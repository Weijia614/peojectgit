{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwGd0xSomAS1LypPfJAsSL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viga132/peojectgit/blob/main/brain%20injury%20senser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "E39xmul1u0hx",
        "outputId": "0a002525-6a32-434f-a4ba-0c7dda2d0458"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'praw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7cc614a5549d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Brain Injury Symptom Extraction from Reddit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'praw'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Brain Injury Symptom Extraction from Reddit\n",
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Reddit API credentials (replace with your own)\n",
        "reddit = praw.Reddit(\n",
        "    client_id='YOUR_CLIENT_ID',\n",
        "    client_secret='YOUR_CLIENT_SECRET',\n",
        "    user_agent='brain_injury_nlp_project'\n",
        ")\n",
        "\n",
        "# Parameters\n",
        "subreddits = ['BrainInjury', 'traumaticbraininjury', 'AskDocs']\n",
        "num_posts = 100\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Collect posts\n",
        "data = []\n",
        "for sub in subreddits:\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    for post in subreddit.hot(limit=num_posts):\n",
        "        if post.selftext:\n",
        "            cleaned = clean_text(post.selftext)\n",
        "            sentiment_scores = sia.polarity_scores(cleaned)\n",
        "            compound = sentiment_scores['compound']\n",
        "            if compound >= 0.05:\n",
        "                sentiment_label = 'positive'\n",
        "            elif compound <= -0.05:\n",
        "                sentiment_label = 'negative'\n",
        "            else:\n",
        "                sentiment_label = 'neutral'\n",
        "\n",
        "            data.append({\n",
        "                'subreddit': sub,\n",
        "                'title': post.title,\n",
        "                'text': post.selftext,\n",
        "                'cleaned_text': cleaned,\n",
        "                'sentiment': compound,\n",
        "                'positive': sentiment_scores['pos'],\n",
        "                'neutral': sentiment_scores['neu'],\n",
        "                'negative': sentiment_scores['neg'],\n",
        "                'sentiment_label': sentiment_label\n",
        "            })\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract symptom-like terms using SpaCy (simplified example)\n",
        "symptom_keywords = ['headache', 'dizzy', 'nausea', 'confusion', 'fatigue', 'memory', 'blurred', 'speech', 'balance']\n",
        "def extract_symptoms(text):\n",
        "    return [word for word in symptom_keywords if word in text]\n",
        "\n",
        "df['symptoms'] = df['cleaned_text'].apply(extract_symptoms)\n",
        "\n",
        "# Visualize most common symptoms\n",
        "all_symptoms = sum(df['symptoms'], [])\n",
        "symptom_counts = pd.Series(all_symptoms).value_counts()\n",
        "plt.figure(figsize=(10,5))\n",
        "symptom_counts.plot(kind='bar')\n",
        "plt.title('Most Common Symptoms Mentioned')\n",
        "plt.xlabel('Symptom')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Generate Word Cloud\n",
        "text_blob = ' '.join(df['cleaned_text'])\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_blob)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Posts')\n",
        "plt.show()\n",
        "\n",
        "# Sentiment Distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['sentiment'], bins=20, kde=True)\n",
        "plt.title('Distribution of Sentiment Scores')\n",
        "plt.xlabel('Compound Sentiment Score')\n",
        "plt.ylabel('Number of Posts')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sentiment Label Distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='sentiment_label', data=df, palette='Set2')\n",
        "plt.title('Distribution of Sentiment Labels')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save data\n",
        "df.to_csv('reddit_brain_injury_posts.csv', index=False)\n",
        "print(\"Data collection and analysis complete.\")\n"
      ]
    }
  ]
}